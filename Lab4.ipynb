{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84771fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ec960",
   "metadata": {},
   "source": [
    "# 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db964500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in array\n",
    "with open(\"framingham.csv\") as file_name:\n",
    "    df = pd.read_csv(file_name, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690ac7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a good practice to make a copy of the \n",
    "# original dataframe before modifying it\n",
    "df_copy = df.copy()\n",
    "df_copy = df_copy.astype(float)\n",
    "df_copy.fillna(df_copy.mean(), inplace=True)\n",
    "column_headers = list(df_copy.columns.values)\n",
    "# Save data in an array\n",
    "df_array = df_copy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ce4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the array to a Pandas DataFrame\n",
    "df = pd.DataFrame(df_array, columns=column_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8f8679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['male',\n",
       " 'age',\n",
       " 'education',\n",
       " 'currentSmoker',\n",
       " 'cigsPerDay',\n",
       " 'BPMeds',\n",
       " 'prevalentStroke',\n",
       " 'prevalentHyp',\n",
       " 'diabetes',\n",
       " 'totChol',\n",
       " 'sysBP',\n",
       " 'diaBP',\n",
       " 'BMI',\n",
       " 'heartRate',\n",
       " 'glucose',\n",
       " 'TenYearCHD']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda3733",
   "metadata": {},
   "source": [
    "# 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d406f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Total variables:\n",
    "'male','age','education','currentSmoker','cigsPerDay','BPMeds','prevalentStroke',\n",
    "'prevalentHyp','diabetes','totChol','sysBP','diaBP','BMI','heartRate','glucose',\n",
    "'TenYearCHD'\n",
    "Variables needed: Unknown, perform forward stepwise\n",
    "Dependent variable: TenYearCHD\n",
    "\n",
    "'''\n",
    "# Drop the \"male\" column\n",
    "df.drop(\"male\", axis=1, inplace=True)\n",
    "# Convert all columns to numeric\n",
    "df = df.apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "# Too many variables, but we need to make sure which ones to use.\n",
    "# Apply stepwise regression to figure out p-value\n",
    "# Define dependent variable and independent variables\n",
    "y = df[\"TenYearCHD\"]\n",
    "X = df.drop([\"TenYearCHD\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4328ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stepwise regression\n",
    "def forward_stepwise(X, y, initial_list=[], threshold_in=0.01, verbose=True):\n",
    "    included = list(initial_list)\n",
    "    excluded = [col for col in X.columns if col not in included]\n",
    "    while True:\n",
    "        changed = False\n",
    "        # Forward step\n",
    "        excluded_dict = {}\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(X[included + [new_column]])).fit()\n",
    "            p_value = model.pvalues[new_column]\n",
    "            excluded_dict[new_column] = p_value\n",
    "        best_column, best_pvalue = min(excluded_dict.items(), key=lambda x: x[1])\n",
    "        if best_pvalue < threshold_in:\n",
    "            included.append(best_column)\n",
    "            excluded.remove(best_column)\n",
    "            if verbose:\n",
    "                print(\"Add {:30} with p-value {:.6}\".format(best_column, best_pvalue))\n",
    "            changed = True\n",
    "        # Termination condition: no more variables can be added\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e362f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add age                            with p-value 6.84501e-50\n",
      "Add sysBP                          with p-value 1.14106e-20\n",
      "Add cigsPerDay                     with p-value 1.20477e-12\n",
      "Add glucose                        with p-value 6.91293e-09\n",
      "Add prevalentStroke                with p-value 0.00196294\n"
     ]
    }
   ],
   "source": [
    "# Run the forward stepwise function \n",
    "# These will be the variables used for the model\n",
    "selected_columns = forward_stepwise(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723c5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the five variables chosen by stepwise regression\n",
    "X = df[['age', 'sysBP', 'cigsPerDay', 'glucose', 'prevalentStroke']].to_numpy()\n",
    "\n",
    "# Extract the target variable\n",
    "y = df['TenYearCHD'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0816d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(y)\n",
    "s_array = s.to_numpy()\n",
    "s_reshaped = s_array.reshape(-1, 1)\n",
    "y = s_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd07d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824cdeb0",
   "metadata": {},
   "source": [
    "# 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc6dbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of a scalar or an array.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc857d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, learning_rate=0.01, num_iterations=1000):\n",
    "    \"\"\"\n",
    "    Implement logistic regression without using any external libraries.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, shape (m, n)\n",
    "    y -- true \"label\" vector (containing 0s and 1s), shape (m, 1)\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    num_iterations -- number of iterations of gradient descent\n",
    "\n",
    "    Returns:\n",
    "    w -- weights, shape (n, 1)\n",
    "    b -- bias, a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize parameters\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    b = 0\n",
    "\n",
    "    # Gradient descent loop\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Forward propagation\n",
    "        z = np.dot(X, w) + b\n",
    "        A = sigmoid(z)\n",
    "        cost = -1/m * np.sum(y*np.log(A) + (1-y)*np.log(1-A))\n",
    "\n",
    "        # Backward propagation\n",
    "        dw = 1/m * np.dot(X.T, (A - y))\n",
    "        db = 1/m * np.sum(A - y)\n",
    "\n",
    "        # Update parameters\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Print cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return w, b\n",
    "\n",
    "def logistic_regression_validation(X, y, learning_rate=0.01, num_iterations=1000):\n",
    "    \"\"\"\n",
    "    Implement logistic regression without using any external libraries.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, shape (m, n)\n",
    "    y -- true \"label\" vector (containing 0s and 1s), shape (m, 1)\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    num_iterations -- number of iterations of gradient descent\n",
    "\n",
    "    Returns:\n",
    "    w -- weights, shape (n, 1)\n",
    "    b -- bias, a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize parameters\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    b = 0\n",
    "\n",
    "    # Gradient descent loop\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Forward propagation\n",
    "        z = np.dot(X, w) + b\n",
    "        A = sigmoid(z)\n",
    "        cost = -1/m * np.sum(y*np.log(A) + (1-y)*np.log(1-A))\n",
    "\n",
    "        # Backward propagation\n",
    "        dw = 1/m * np.dot(X.T, (A - y))\n",
    "        db = 1/m * np.sum(A - y)\n",
    "\n",
    "        # Update parameters\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94e162e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.453698\n",
      "Cost after iteration 200: 0.453632\n",
      "Cost after iteration 300: 0.453595\n",
      "Cost after iteration 400: 0.453573\n",
      "Cost after iteration 500: 0.453558\n",
      "Cost after iteration 600: 0.453548\n",
      "Cost after iteration 700: 0.453541\n",
      "Cost after iteration 800: 0.453536\n",
      "Cost after iteration 900: 0.453531\n"
     ]
    }
   ],
   "source": [
    "weights, bias = logistic_regression(X_train, y_train, learning_rate=0.0001, num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65176b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.37735849056604 %\n"
     ]
    }
   ],
   "source": [
    "def predict(X, weights, bias):\n",
    "    z = np.dot(X, weights) + bias\n",
    "    y_pred = sigmoid(z)\n",
    "    y_pred_class = (y_pred > 0.5).astype(int)\n",
    "    return y_pred_class\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = predict(X_test, weights, bias)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = (1 - np.mean(np.abs(y_pred - y_test))) * 100\n",
    "\n",
    "print(\"Accuracy:\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5046d4",
   "metadata": {},
   "source": [
    "# 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6785a25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 0: 0.693147\n",
      "Average accuracy: 84.80419237709118 %\n"
     ]
    }
   ],
   "source": [
    "# Define the number of folds\n",
    "k = 5\n",
    "\n",
    "# Split the data into k folds\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "scores = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train the logistic regression model\n",
    "    weights, bias = logistic_regression(X_train, y_train, learning_rate=0.0001, num_iterations=100)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = predict(X_test, weights, bias)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    accuracy = (1 - np.mean(np.abs(y_pred - y_test))) * 100\n",
    "    \n",
    "    # Append the accuracy score to the list\n",
    "    scores.append(accuracy)\n",
    "\n",
    "# Compute the average accuracy over all folds\n",
    "avg_accuracy = np.mean(scores)\n",
    "\n",
    "print(\"Average accuracy:\", avg_accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c90b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_poly(X, y, k=5, learning_rate=0.01, num_iters=1000, degree_range=range(1, 6)):\n",
    "    m, n = X.shape\n",
    "    fold_size = int(m/k)\n",
    "    best_degree = None\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for degree in degree_range:\n",
    "        X_poly = np.ones((m, 1))\n",
    "        for d in range(1, degree+1):\n",
    "            X_poly = np.hstack((X_poly, np.power(X, d)))\n",
    "            \n",
    "        for i in range(k):\n",
    "            X_val = X_poly[i*fold_size:(i+1)*fold_size, :]\n",
    "            y_val = y[i*fold_size:(i+1)*fold_size]\n",
    "            X_train = np.vstack((X_poly[:i*fold_size, :], X_poly[(i+1)*fold_size:, :]))\n",
    "            y_train = np.concatenate((y[:i*fold_size], y[(i+1)*fold_size:]))\n",
    "            \n",
    "            weights, bias = logistic_regression_validation(X_train, y_train, learning_rate, num_iters)\n",
    "            \n",
    "            y_pred = predict(X_val, weights, bias)\n",
    "            \n",
    "            accuracy = np.mean(y_pred == y_val)\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_degree = degree\n",
    "                best_accuracy = accuracy\n",
    "                \n",
    "    return best_degree, best_accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "340e8c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_2652\\2804211525.py:69: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -1/m * np.sum(y*np.log(A) + (1-y)*np.log(1-A))\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_2652\\2804211525.py:69: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -1/m * np.sum(y*np.log(A) + (1-y)*np.log(1-A))\n",
      "C:\\Users\\Gamer\\AppData\\Local\\Temp\\ipykernel_2652\\2469156700.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Degree:  5\n",
      "Best Accuracy:  0.8654073199527745\n"
     ]
    }
   ],
   "source": [
    "best_degree, best_accuracy = cross_validate_poly(X, y)\n",
    "print('Best Degree: ', best_degree)\n",
    "print('Best Accuracy: ', best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17d2ea",
   "metadata": {},
   "source": [
    "# 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0519c29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La regresion logistica aplicada al modelo fue efectiva, tieniedo un acierto del 85%. La validacion cruzada, con un grado de polinomio 5, tuvo un acierto del 86%. Las 5 variables usadas para este modelo fueron elegidas por un forward stepwise, una tecnica usada en modelos estadisticos para encontrar el mejor subset de variables predictoras para un modelo de regresion lineal. La meta es poder encontrar las variables que aporten mejor rendimiento de prediccion, y entre menos variables, mejor. Es importante mencionar que el 85% a 86% de acierto, en el campo de medicina, es bastante bajo. La vida de personas depende de estos modelos de predicciones. Pero en general, el concepto de la regresion logistica fue aprendido y puesto en practica de manera efectiva. Sin embargo, hay librerias de sklearn que hacen de esta regresion logistica mas efectiva y exacta.\n"
     ]
    }
   ],
   "source": [
    "print(\"La regresion logistica aplicada al modelo fue efectiva, tieniedo un \\\n",
    "acierto del 85%. La validacion cruzada, con un grado de polinomio 5, \\\n",
    "tuvo un acierto del 86%. Las 5 variables usadas para este modelo \\\n",
    "fueron elegidas por un forward stepwise, una tecnica usada en modelos \\\n",
    "estadisticos para encontrar el mejor subset de variables predictoras para \\\n",
    "un modelo de regresion lineal. La meta es poder encontrar las variables \\\n",
    "que aporten mejor rendimiento de prediccion, y entre menos variables, mejor. \\\n",
    "Es importante mencionar que el 85% a 86% de acierto, en el campo de medicina, \\\n",
    "es bastante bajo. La vida de personas depende de estos modelos de \\\n",
    "predicciones. Pero en general, el concepto de la regresion logistica \\\n",
    "fue aprendido y puesto en practica de manera efectiva. Sin embargo, hay \\\n",
    "librerias de sklearn que hacen de esta regresion logistica mas efectiva \\\n",
    "y exacta.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
